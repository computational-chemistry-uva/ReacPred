{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red26\green26\blue26;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c13333\c13333\c13333;\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Data\

\f1\b0 - Data files\
The data is stored in the folder Data; there are folders 
\f0\b D1
\f1\b0 , 
\f0\b D2
\f1\b0 , 
\f0\b Dm
\f1\b0  that correspond to the DFT generated data, experimental data and the merged dataset respectively in accordance with the definitions used in the article. Each of these folders contains three .csv files that corresponds three different fitnesses, e.g.:\
\
d1_f_Ev.csv - fitness is volumetric energy density \
d1_f_H.csv - fitness is hydration enthalpy\
d1_f_rho.csv - fitness is density\
\
The filenames start with d2 and with dm for D2 and Dm datasets respectively;\
In addition, D1 and Dm folders contain folder called \'91Folds\'92; this folder contains train and test folds used for 10-fold cross-validation for each of the fitnesses;  \
\
If needed, an alternative split can be generated using the script 
\f0\b create_folds.py 
\f1\b0 and one of the files with the data that needs to be split.\
\
- Data format\
If it is needed to create a custom dataset, the .csv file must have a particular format\
Column 1: name - \'91idx\'92, values - integers; represents number of a data point\
Column 2: name - \'91Base salt\'92, values - strings; represents salt composition; \
Column 3: name - \'91Initial loading\'92, values - float; represents amount of water in the initial hydrate\
Column 4: name - \'91Final loading\'92, values - float; represents amount of water molecules in the product hydrate\
Column 5: name - \'91Fitness\'92, values - float; represents the target value of the predicted property\
Column 6: name - \'91Origin\'92, values - optional; represents origin of the data; if this column is not present a dummy column will be created; else, the provided origins will be replaced by numeric values, the replacement rule will be printed\

\f0\b \
\

\f1\b0 Remarks
\f0\b \

\f1\b0 1) If a dataset is going to be used only for predictions with a retrained model and the target value is unknown - the \'92Fitness\'92 column must still be present and contain some dummy numerical values (e.g. zeros)\
2) Despite the names \'91Base salt\'92, \'91Initial loading\'92, \'91Final loading\'92 are related to salt hydration reaction, the models can be applied to any addition reaction of type S.mL + (n-m)L -> S.nL where S is a composition of a substrate, n and m are stoichiometric amounts of a certain ligand L (identical for the dataset)\
\

\f0\b Embeddings\

\f1\b0 The folder embeddings contains element embeddings as reported in the original Roost paper \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \expnd0\expndtw0\kerning0
(Goodall, R.E.A., Lee, A.A,\'a0 
\f2\i \cb1 Nat Commun
\f1\i0 \cb3  \cb1 11\cb3 , 6280 (2020))\
Additionally, it contains Magpie features for data used for model training on Dm data (magpie_features.json);  this file can also be used to run tasks on the D1 data only \
If 
\f0\b MagPieNN (ReacCryNN)
\f1\b0  predictions/training is desired for some compositions that are 
\f0\b not
\f1\b0  in that data, new Magpie features must be generated before running the model using the script 
\f0\b create_magpie.py\
\
(Quick) Setup\

\f1\b0 1)
\f0\b  
\f1\b0 Setup a Python environment. We recommend using Python 3.9.19, as this version has been used throughout the project \
If you use conda, create a new environment, and activate it in Terminal:\
conda create -n 
\f2\i env_name
\f1\i0  python=3.9.19\
conda activate 
\f2\i env_name
\f1\i0 \
\
2) Instal libraries - the libraries and their versions are saved in the requirements.txt.\
In conda, after creating and activating the environment, install requirements using pip
\f0\b \

\f1\b0 pip install -r requirements.txt\
\
3) In your IDE open the file workspace.py, and run the code. If everything is set up right, this should start a single train/test split run using ReacRoostC on the d1 dataset, for getting the model predicting densities. \
\
4) The prediction results will be saved in the folder results, and the model will be saved in the folder model
\f0\b \
\
\
Running tasks\
Arguments\

\f1\b0 In the file 
\f2\i workspace.py
\f1\i0  there is a function called input_parser() which is generating the arguments for task initialisation with default values\

\f2\i args = input_parser()
\f1\i0 \
This can be used to run tasks from Terminal, as designed in the original paper\
\
We use an alternative approach, namely running tasks from an IDE\
Firstly, args are converted into a Python dict by:\

\f2\i args = vars(args)
\f1\i0 \
This dict can be printed if it\'92s necessary to see the argument keywords and default values, alternatively the 
\f2\i input_parcer()
\f1\i0  can be examined;\
\
The args can be modified with custom values using arguments\'92 names as dict keys\
Some commonly modified arguments are:\
\

\f2\i args['train'] = bool
\f1\i0  - specifies whether the model has to be trained or not; if it\'92s False, a checkpoint with a prerained model is required\
\

\f2\i args[\'91evaluate\'92] = bool
\f1\i0  - specifies whether a model has to be evaluated on an independent data set and save results into the Results directory; the path to the test set can be provided (see input_parser); if not evaluates data on the validation set\
\

\f2\i args['fine_tune'] = bool 
\f1\i0 - specifies path for the checkpoint of a pretrained model; checkpoints are saved in the 
\f2\i models 
\f1\i0 directory; it looks like /../models/roost_10_epochs/checkpoint-r0.pth.tar\'92; NOTE, if 
\f2\i train
\f1\i0  is 
\f2\i True
\f1\i0 , the model will be retrained, so if only evaluation on a test set is needed is needed, use 
\f2\i args['train'] = False
\f1\i0 \
\

\f2\i args['data_path'] = str - 
\f1\i0 specifies path to data, by uses 0.8 of the data for training and 0.2 for validation/test if no
\f2\i  test_path
\f1\i0  is provided\
\

\f2\i args[\'91test_path\'92] = str - 
\f1\i0 specifies path to the test set; if provided and validation set is not specified uses the test set for validation during the training; if not provided by default uses 0.2 of the data in the data_path as a validation and test set\
\

\f2\i args[\'91epoch\'92] =  int - 
\f1\i0 specifies number of epochs, i.e. full passes through training data\
\

\f2\i args['model_class'] = str
\f1\i0  - specifies model type, can be only one of 
\f2\i ReacCryNet, ReacElemNet, ReacRoost
\f1\i0 \

\f2\i args['fea_path'] = str 
\f1\i0 - specifies a path to the features/embeddings; if 
\f2\i ReacCryNet
\f1\i0  is used as model class - requires a path to crystal features; if 
\f2\i ReacElemNet or ReacRoost 
\f1\i0 is used - requires path to element embeddings\
\

\f2\i args['elem_fea_len'] = int - 
\f1\i0 specifies dimension of the feature/element embedding vector; if dimension reduction is not used (see below), must be the same as the original feature/embedding length;\

\f2\i args[\'91append_after] = str - 
\f1\i0 specifies whether loadings are appended to the element embeddings before (\'91E\'92) or after (\'91C\'92) message passing operations. Makes a difference only for model type ReacRoost, for others it\'92s always \'91C\'92;\
\

\f2\i args['dim_red'] = bool - 
\f1\i0 specifies, if learnable dimension reduction is performed on features/embeddings before message passing and/or predictive model;\
if 
\f2\i True 
\f1\i0 elem_fea_len must be < elem_emb_len, \
if 
\f2\i True
\f1\i0 , in case model_class is 
\f2\i ReacCryNet
\f1\i0 , 
\f2\i elem_fea_len 
\f1\i0 corresponds to the embedding length BEFORE appending loadings, i.e. the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len + 2;
\f1\i0 \
if 
\f2\i True
\f1\i0 , in case model_class is 
\f2\i ReacElemNet
\f1\i0 , 
\f2\i elem_fea_len 
\f1\i0 corresponds to embedding length BEFORE appending loadings, i.e. the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len + 2;\

\f1\i0 if 
\f2\i True
\f1\i0 , in case model_class is 
\f2\i ReacRoost
\f1\i0  
\f0\b AND
\f1\b0  append_after = \'91C\'92 
\f2\i elem_fea_len 
\f1\i0 corresponds to embedding length BEFORE appending the loadings, but AFTER appending the atomic fraction (weight), i.e. the dimension of feature after the dimension reduction step is 
\f2\i elem_fea_len -1, 
\f1\i0 the dimension of feature
\f2\i  
\f1\i0 after appending the atomic fraction (weight) and in the message passing section is 
\f2\i elem_fea_len, 
\f1\i0 and eventually the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len + 2\

\f1\i0 if 
\f2\i True
\f1\i0 , in case model_class is 
\f2\i ReacRoost
\f1\i0  
\f0\b AND
\f1\b0  append_after = \'91E\'92 
\f2\i elem_fea_len 
\f1\i0 corresponds to embedding length AFTER appending  BOTH the loadings the atomic fractions (weights), i.e. the dimension of feature after the dimension reduction step is 
\f2\i elem_fea_len - 3, 
\f1\i0 the dimension of feature
\f2\i  
\f1\i0 after appending both the atomic fraction (weight) and the loadings and consequently in the message passing section is 
\f2\i elem_fea_len, 
\f1\i0 and eventually the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len\

\f1\i0 if 
\f2\i False
\f1\i0 , in case model_class is 
\f2\i ReacRoost
\f1\i0  
\f0\b AND 
\f1\b0 append_after = 
\f2\i \'91C\'92 elem_fea_len 
\f1\i0 corresponds to the original embedding length, i.e. the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len + 3 (two loadings + one atomic fraction (weight))\

\f1\i0 if 
\f2\i False
\f1\i0 , in case model_class is ReacRoost 
\f0\b AND 
\f1\b0 append_after = \'91E\'92 
\f2\i elem_fea_len 
\f1\i0 corresponds to the original embedding length, i.e. the dimension of feature used as input of final predictive model is 
\f2\i elem_fea_len + 3 (two loadings + one atomic fraction (weight))
\f1\i0 \
\

\f0\b Running
\f1\b0 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 The tasks are run by calling main(**args) in the 
\f0\b \cf2 \cb3 workspace.py
\f1\b0 \cf0 \cb1 .\
A single train/test split run or an evaluation of a preretrained on an independent test set is done by a single function call (see #Running a task on a single train/test split in 
\f0\b workspace.py
\f1\b0 ). The test results are saved in the Results directory.\
A k-fold cross-validation task is run in a loop of k iterations (see #Running a cross-validation task in 
\f0\b workspace.py
\f1\b0 ). Before running, k train test splits of data must be created, which can be done using script 
\f0\b create_folds.py.
\f1\b0  The model parameters and the path to the directory with the splits (folds) are specified before looping. In the loop, the train/test splits and model names are alternated at each iteration and the results are saved in the Results folder. The results can be analysed using the 
\f0\b analyse_CV.py
\f1\b0  script. NOTE the Results are not necessarily overwritten, so before analysing cross-validation results, make sure, the folder contains only the necessary files with the results.\
\
Remarks\
Make sure to comment/uncomment unnecessary parts: the workspace can run either a single train/test split task or a k-fold cross-validation. A prediction with a retrained model without training can be set up in the a single train/test split, by disabling training (
\f2\i \cf2 \cb3 args['train'] = False
\f1\i0 \cf0 \cb1 ), providing paths to a checkpoint with a pre-trained model (
\f2\i \cf2 \cb3 args['fine_tune'] = \'91\'92path\'92) 
\f1\i0 and to an independent test set (
\f2\i args[\'91test_path\'92] = path
\f1\i0 )}